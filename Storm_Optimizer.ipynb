{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python38264bita9fe091a7e544366b49cd42eea3180f6",
      "display_name": "Python 3.8.2 64-bit"
    },
    "colab": {
      "name": "Storm Optimizer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshank528/Project-STORM/blob/master/Storm_Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC8kR2aRmH3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating STORM optimizer class as per algorithm in the paper https://arxiv.org/abs/1905.10018\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "class StormOptimizer(Optimizer):    \n",
        "    # Storing the parameters required in defaults dictionary\n",
        "    # lr-->learning rate\n",
        "    # c-->parameter to be swept over logarithmically spaced grid as per paper\n",
        "    # w and k to be set as 0.1 as per paper\n",
        "    # momentum-->dictionary storing model params as keys and their momentum term as values at each iteration(denoted by 'd' in paper)\n",
        "    # gradient--> dictionary storing model params as keys and their gradients till now in a list as values(denoted by '∇f(x,ε)' in paper)\n",
        "    # sqrgradnorm-->dictionary storing model params as keys and their sum of norm ofgradients till now as values(denoted by '∑G^2' in paper)\n",
        "\n",
        "    def __init__(self,params,lr=0.1,c=100,momentum={},gradient={},sqrgradnorm={}):\n",
        "        defaults = dict(lr=lr,c=c,momentum=momentum,sqrgradnorm=sqrgradnorm,gradient=gradient)\n",
        "        super(StormOptimizer,self).__init__(params,defaults)\n",
        "\n",
        "    # Returns the state of the optimizer as a dictionary containing state and param_groups as keys\n",
        "    def __setstate__(self,state):\n",
        "        super(StormOptimizer,self).__setstate__(state)\n",
        "\n",
        "    # Performs a single optimization step for parameter updates\n",
        "    def step(self,closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        # param_groups-->a dict containing all parameter groups\n",
        "        for group in self.param_groups:\n",
        "           # Retrieving from defaults dictionary\n",
        "           learn_rate = group['lr']\n",
        "           factor = group['c']\n",
        "           momentum = group['momentum']\n",
        "           gradient = group['gradient']\n",
        "           sqrgradnorm = group['sqrgradnorm']\n",
        "\n",
        "           # Update step for each parameter present in param_groups\n",
        "           for p in group['params']:\n",
        "                # Calculating gradient('∇f(x,ε)' in paper)\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "\n",
        "                # Storing all gradients in a list\n",
        "                if p in gradient:\n",
        "                    gradient[p].append(dp)\n",
        "                else:\n",
        "                    gradient.update({p:[dp]})\n",
        "\n",
        "                # Calculating and storing ∑G^2in sqrgradnorm\n",
        "                if p in sqrgradnorm:\n",
        "                    sqrgradnorm[p] = sqrgradnorm[p] + torch.pow(torch.norm(dp),2)\n",
        "                else:\n",
        "                    sqrgradnorm.update({p:torch.pow(torch.norm(dp),2)})\n",
        "\n",
        "                # Updating learning rate('η' in paper)\n",
        "                power = 1.0/3.0\n",
        "                scaling = torch.pow((0.1 + sqrgradnorm[p]),power)\n",
        "                learn_rate = learn_rate/(float)(scaling)\n",
        "\n",
        "                # Calculating 'a' mentioned as a=cη^2 in paper(denoted 'c' as factor here)\n",
        "                a = min(factor*learn_rate**2.0,1.0)\n",
        "\n",
        "                # Calculating and storing the momentum term(d'=∇f(x',ε')+(1-a')(d-∇f(x,ε')))\n",
        "                if p in momentum:\n",
        "                    momentum[p] = gradient[p][-1] + (1-a)*(momentum[p]-gradient[p][-2])\n",
        "                else:\n",
        "                    momentum.update({p:dp})\n",
        "\n",
        "                # Updation of model parameter p                \n",
        "                p.data = p.data-learn_rate*momentum[p]\n",
        "                learn_rate = group['lr']\n",
        "        \n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyxDWQEhmH3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}